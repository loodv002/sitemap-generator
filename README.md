# Sitemap Generator

## Third-party libraries

本專案使用以下三個第三方套件:
* requests
* cupy
* numpy

可透過以下指令安裝:
```shell
$ pip install -r requirements.txt
```

## Quick Start

以下為各部件的範例，完整範例程式請見 [example.py](example.py)。

### Crawler
```python
from crawler import *
cr = Crawler(CrawlerConfig(
    index_urls=['https://info.nycu.edu.tw/'],
    allow_domains=['*info.nycu.edu.tw'],
))
cr.main()
```
`CrawlerConfig` 具體設定請見 [Crawler設定說明](crawler/README.md)，生成測試資料所用的設定請見 [crawler_test_config.py](crawler_test_config.py)。

### Analyzer
```python
# cr is a Crawler instance.
from analyzer import *
az = Analyzer(graph_manager=cr.graph_manager, config=AnalyzerConfig())
urlweight = az.main()
```

範例中的 `cr.graph_manager` 是一個 `crawler.GraphManager` 實例，Crawler 執行完後，連結圖會儲存其中。你也可以透過 `data/graph/` 中的檔案，載入爬取好的網站資料，如下:

```python
from crawler import *
gm = GraphManager.load_from_file(f'data/graph/docs.python.org_only313.pkl')
# az = Analyzer(graph_manager=gm, ...
```

`AnalyzerConfig` 之設定與實驗重現部分請見 [Analyer 說明](analyzer/README.md)。

### Generator
Serial 版本範例:
```python
# `urlweight`is a list of urls and priorities, generated by `Analyzer.main`.
sm = SiteMap()
sm.main(urlweight) 
```

Parallel 版本範例:
```python
sm = SiteMapParallel()
sm.main(urlweight, 2) # Run in 2 threads
```

其中的 `urlweight` 是 `Analyzer.main` 生成的權重，也可以透過以下程式載入儲存在 `data/generator` 裡的權重:
```python
import pickle
with open('data/generator/urlweight_docs.python.org_only313.pkl', 'rb') as file:
    urlweight = pickle.load(file)
```

`Generator` 之實驗重現部分請見 [Generator 說明](generator/README.md)。